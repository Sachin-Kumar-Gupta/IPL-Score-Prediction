# -*- coding: utf-8 -*-
"""IPL Score Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pUPWZBWfVqxbIXMyo7d9vJz4OduQsd_u
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

data = pd.read_excel("IPL_Data.xlsx")
data.head(5)

data.info()

data.describe()

print("We have an IPL data from {} to {} ".format(data['date'].dt.year.min(), data['date'].dt.year.max()))

data.isnull().sum()

data['bat_team'].unique()

data['bowl_team'].unique()

"""Droping teams who don't play now."""

data['bowl_team'].value_counts()

teams = ['Kolkata Knight Riders', 'Chennai Super Kings', 'Rajasthan Royals','Mumbai Indians',
         'Kings XI Punjab', 'Royal Challengers Bangalore', 'Delhi Daredevils', 'Sunrisers Hyderabad']

data = data[(data['bat_team'].isin(teams))&(data['bowl_team'].isin(teams))]

"""For final score predictions we don't need batsman, bowler, stricker, non-stricker and mid columns. So drop them."""

data.drop(['mid','batsman','bowler','striker','non-striker'], axis=1,inplace=True)
data.head(5)

corr = data.corr()
sns.heatmap(corr,annot=True)

# Converting categorical features using OneHotEncoding method
df = pd.get_dummies(data=data, columns=['bat_team', 'bowl_team','venue'])

df.head(5)

# Splitting the data into train and test set
X_train = df.drop(labels='total', axis=1)[df['date'].dt.year <= 2016]
X_test = df.drop(labels='total', axis=1)[df['date'].dt.year >= 2017]

y_train = df[df['date'].dt.year <= 2016]['total'].values
y_test = df[df['date'].dt.year >= 2017]['total'].values

# Removing the 'date' column
X_train.drop(['date'], axis=True, inplace=True)
X_test.drop(['date'], axis=True, inplace=True)

"""# Model Building"""

# Linear Regression Model
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor.fit(X_train,y_train)

y_pred = regressor.predict(X_test)

from sklearn.metrics import r2_score, mean_absolute_error

mean_absolute_error(y_test,y_pred)

r2_score(y_test,y_pred)

from sklearn.ensemble import RandomForestRegressor
rf = RandomForestRegressor()
rf.fit(X_train,y_train)

rf_pred = rf.predict(X_test)

r2_score(y_test,rf_pred)

## Ridge Regression
from sklearn.linear_model import Ridge
from sklearn.model_selection import GridSearchCV

ridge=Ridge()
parameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40]}
ridge_regressor=GridSearchCV(ridge,parameters,scoring='neg_mean_squared_error',cv=5)
ridge_regressor.fit(X_train,y_train)

rg_pred = ridge_regressor.predict(X_test)
r2_score(y_test,rg_pred)

from xgboost import XGBRegressor
xgb = XGBRegressor()
xgb.fit(X_train,y_train) 
xgb_pred = xgb.predict(X_test)

r2_score(y_test,xgb_pred)

# Creating a pickle file for the classifier
import pickle
filename = 'model.pkl'
pickle.dump(ridge_regressor, open(filename, 'wb'))









































